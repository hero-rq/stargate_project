
# **Stargate Setup & Running Jobs on HPC**  

## **1Ô∏è‚É£ Connecting to VPN & Authenticating**  
Ensure you are connected to your VPN and authenticated before proceeding.  

## **2Ô∏è‚É£ Start an Interactive HPC Session**  
```bash
srun --pty bash -i
```

## **3Ô∏è‚É£ Load Required Modules**  
```bash
module load Java/17.0.4
module load Anaconda3/2024.02-1
source activate myspark
export LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8
pip install matplotlib pandas
```

## **4Ô∏è‚É£ Navigate to the Project Directory**  
```bash
cd com6012/ScalableML/
```

---

## **5Ô∏è‚É£ Writing a Python Script in `Code/` Directory**  
Place your Python script inside the `Code/` directory to be executed within the HPC environment.

---

## **6Ô∏è‚É£ Submitting a Batch Job**  

Create a job script **`HPC/initial_top_down.sh`**:  

```bash
#!/bin/bash
#SBATCH --job-name=panda_kumo   # Replace with a meaningful job name
#SBATCH --time=00:30:00         # Adjust the runtime if needed
#SBATCH --nodes=1               # Number of nodes requested
#SBATCH --mem=4G                # Memory allocation (4GB)
#SBATCH --output=./Output/initial_topdown_state.txt  # Log output location

module load Java/17.0.4
module load Anaconda3/2024.02-1

source activate myspark
```

### **Submit the Job**  
```bash
sbatch HPC/initial_top_down.sh
```
‚úîÔ∏è This will return a **job ID**, e.g.:
```
Submitted batch job 5542113
```

---

## **7Ô∏è‚É£ Checking the Job Output**  
```bash
cd Output
ls
```
Example output:
```
COM6012_Lab1_SAMPLE.txt  COM6012_Lab1.txt  COM6012_Lab2.txt  
initial_topdown_state.txt  output_27_0.png
```

To view the batch job log:
```bash
cat initial_topdown_state.txt
```

---

### **‚úÖ Done!**  
Now your job is executed, and you can check the results inside the **`Output/`** directory. üéØ

Let me know if you want to tweak anything! üòÉüî•
